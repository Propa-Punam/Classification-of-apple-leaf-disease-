{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":658267,"sourceType":"datasetVersion","datasetId":277323}],"dockerImageVersionId":30746,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\n\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-08-12T07:16:34.743062Z","iopub.execute_input":"2024-08-12T07:16:34.743556Z","iopub.status.idle":"2024-08-12T07:16:34.751347Z","shell.execute_reply.started":"2024-08-12T07:16:34.743465Z","shell.execute_reply":"2024-08-12T07:16:34.749679Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"import os\nimport numpy as np\nimport pandas as pd\nfrom sklearn.metrics import classification_report, accuracy_score, confusion_matrix\nimport torch\nimport torch.nn as nn\nimport torchvision.transforms as transforms\nfrom PIL import Image\nfrom torchvision import models\nimport timm\nfrom tqdm import tqdm\nfrom sklearn.model_selection import train_test_split, GridSearchCV\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.decomposition import PCA\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.utils.class_weight import compute_class_weight\nfrom torch.utils.data import DataLoader, Dataset\n\n# Define the ResNet-ViT hybrid model with Dropout\nclass ResNetViT(nn.Module):\n    def __init__(self):\n        super(ResNetViT, self).__init__()\n        # Load pre-trained ResNet50\n        self.resnet = models.resnet50(pretrained=True)\n        self.resnet.fc = nn.Identity()  # Remove the classification layer\n\n        # Load pre-trained Vision Transformer (ViT)\n        self.vit = timm.create_model('vit_base_patch16_224', pretrained=True)\n        self.vit.head = nn.Identity()  # Remove the classification layer\n\n        self.dropout = nn.Dropout(0.5)  # Dropout with a 50% rate\n\n    def forward(self, x):\n        x_resnet = self.resnet(x)\n        x_vit = self.vit(x)\n        x = torch.cat((x_resnet, x_vit), dim=1)\n        x = self.dropout(x)  # Apply dropout\n        return x\n\n# Instantiate the model\nmodel = ResNetViT()\nmodel.eval()  # Set to evaluation mode\n\n# Define image preprocessing with data augmentation\npreprocess = transforms.Compose([\n    transforms.Resize((224, 224)),\n    transforms.RandomHorizontalFlip(),  # Randomly flip images horizontally\n    transforms.RandomVerticalFlip(),    # Randomly flip images vertically\n    transforms.RandomRotation(30),      # Randomly rotate images\n    transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.2),  # Randomly change the brightness, contrast, saturation and hue\n    transforms.RandomGrayscale(p=0.2),  # Randomly convert images to grayscale\n    transforms.ToTensor(),\n    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n])\n\n# Define custom dataset\nclass AppleDataset(Dataset):\n    def __init__(self, image_paths, labels, preprocess):\n        self.image_paths = image_paths\n        self.labels = labels\n        self.preprocess = preprocess\n\n    def __len__(self):\n        return len(self.image_paths)\n\n    def __getitem__(self, idx):\n        img_path = self.image_paths[idx]\n        img = Image.open(img_path).convert('RGB')\n        img = self.preprocess(img)\n        label = self.labels[idx]\n        return img, label\n\n# Function to extract features\ndef extract_features(model, data_loader):\n    model.eval()\n    features = []\n    labels = []\n    with torch.no_grad():\n        for imgs, lbls in tqdm(data_loader):\n            features_batch = model(imgs)\n            features.append(features_batch.numpy())\n            labels.extend(lbls)\n    return np.vstack(features), np.array(labels)\n\n# Create dataset\ndef create_image_paths_dataset(paths: list, labels: list):\n    image_paths = []\n    image_labels = []\n    for path, label in zip(paths, labels):\n        files = [f for f in os.listdir(path) if os.path.isfile(os.path.join(path, f))]\n        image_paths.extend([os.path.join(path, f) for f in files])\n        image_labels.extend([label] * len(files))\n    \n    return pd.DataFrame({'image_path': image_paths, 'label': image_labels})\n\n# Paths to datasets\nscab_path = '/kaggle/input/plantvillage-dataset/color/Apple___Apple_scab/'\nblack_path = '/kaggle/input/plantvillage-dataset/color/Apple___Black_rot/'\nrust_path = '/kaggle/input/plantvillage-dataset/color/Apple___Cedar_apple_rust/'\nhealthy_path = '/kaggle/input/plantvillage-dataset/color/Apple___healthy/'\n\npaths = [scab_path, black_path, rust_path, healthy_path]\nlabels = ['scab', 'black_rot', 'cedar_rust', 'healthy']\napples = create_image_paths_dataset(paths, labels)\n\n# Split the data into training, validation, and testing sets\nimage_paths = apples['image_path'].tolist()\nlabels = apples['label'].tolist()\nX_train_paths, X_temp_paths, y_train, y_temp = train_test_split(image_paths, labels, test_size=0.3, random_state=42, stratify=labels)\nX_val_paths, X_test_paths, y_val, y_test = train_test_split(X_temp_paths, y_temp, test_size=0.5, random_state=42, stratify=y_temp)\n\n# Create data loaders\ntrain_dataset = AppleDataset(X_train_paths, y_train, preprocess)\nval_dataset = AppleDataset(X_val_paths, y_val, preprocess)\ntest_dataset = AppleDataset(X_test_paths, y_test, preprocess)\n\ntrain_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\nval_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)\ntest_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n\n# Extract features from the training, validation, and testing sets\nfeatures_train, labels_train = extract_features(model, train_loader)\nfeatures_val, labels_val = extract_features(model, val_loader)\nfeatures_test, labels_test = extract_features(model, test_loader)\n\n# Standardize the features\nscaler = StandardScaler()\nfeatures_train_scaled = scaler.fit_transform(features_train)\nfeatures_val_scaled = scaler.transform(features_val)\nfeatures_test_scaled = scaler.transform(features_test)\n\n# Apply PCA for feature selection\npca = PCA(n_components=10)  # Adjust the number of components as needed\nfeatures_train_reduced = pca.fit_transform(features_train_scaled)\nfeatures_val_reduced = pca.transform(features_val_scaled)\nfeatures_test_reduced = pca.transform(features_test_scaled)\n\n# Compute class weights\nclass_weights = compute_class_weight('balanced', classes=np.unique(labels_train), y=labels_train)\nclass_weights_dict = {cls: weight for cls, weight in zip(np.unique(labels_train), class_weights)}\n\n# Initialize logistic regression model with class weights\nlogistic_regression = LogisticRegression(max_iter=1000, penalty='l2', C=0.1, class_weight=class_weights_dict)\n\n# Hyperparameter tuning using Grid Search\nparam_grid = {'C': [0.01, 0.1, 1, 10, 100]}\ngrid_search = GridSearchCV(logistic_regression, param_grid, cv=5, scoring='accuracy')\ngrid_search.fit(features_train_reduced, labels_train)\nbest_model = grid_search.best_estimator_\n\n# Evaluate on validation set\nval_pred = best_model.predict(features_val_reduced)\nval_accuracy = accuracy_score(labels_val, val_pred)\nval_conf_matrix = confusion_matrix(labels_val, val_pred)\nval_class_report = classification_report(labels_val, val_pred)\n\nprint(f\"Validation Accuracy: {val_accuracy}\")\nprint(\"Validation Confusion Matrix:\")\nprint(val_conf_matrix)\nprint(\"Validation Classification Report:\")\nprint(val_class_report)\n\n# Make predictions on the test set using the best model from GridSearchCV\ny_pred = best_model.predict(features_test_reduced)\n\n# Evaluate the model on the test set\naccuracy = accuracy_score(labels_test, y_pred)\nconf_matrix = confusion_matrix(labels_test, y_pred)\nclass_report = classification_report(labels_test, y_pred, output_dict=True)\nmetrics = pd.DataFrame(class_report).transpose()\n\nprint(f\"Test Accuracy: {accuracy}\")\nprint(\"Test Confusion Matrix:\")\nprint(conf_matrix)\nprint(\"Test Classification Report:\")\nprint(metrics)\n","metadata":{"execution":{"iopub.status.busy":"2024-08-12T07:16:34.843031Z","iopub.execute_input":"2024-08-12T07:16:34.843518Z","iopub.status.idle":"2024-08-12T07:41:22.441031Z","shell.execute_reply.started":"2024-08-12T07:16:34.843479Z","shell.execute_reply":"2024-08-12T07:41:22.439696Z"},"trusted":true},"execution_count":6,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n  warnings.warn(msg)\n100%|██████████| 70/70 [17:56<00:00, 15.38s/it]\n100%|██████████| 15/15 [03:23<00:00, 13.57s/it]\n100%|██████████| 15/15 [03:17<00:00, 13.19s/it]\n","output_type":"stream"},{"name":"stdout","text":"Validation Accuracy: 0.8529411764705882\nValidation Confusion Matrix:\n[[ 85   1   4   3]\n [  2  35   1   3]\n [ 12   3 216  16]\n [ 10   8   7  70]]\nValidation Classification Report:\n              precision    recall  f1-score   support\n\n   black_rot       0.78      0.91      0.84        93\n  cedar_rust       0.74      0.85      0.80        41\n     healthy       0.95      0.87      0.91       247\n        scab       0.76      0.74      0.75        95\n\n    accuracy                           0.85       476\n   macro avg       0.81      0.84      0.82       476\nweighted avg       0.86      0.85      0.85       476\n\nTest Accuracy: 0.8760504201680672\nTest Confusion Matrix:\n[[ 82   1   6   4]\n [  1  38   0   3]\n [  7   0 218  22]\n [  5   4   6  79]]\nTest Classification Report:\n              precision    recall  f1-score    support\nblack_rot      0.863158  0.881720  0.872340   93.00000\ncedar_rust     0.883721  0.904762  0.894118   42.00000\nhealthy        0.947826  0.882591  0.914046  247.00000\nscab           0.731481  0.840426  0.782178   94.00000\naccuracy       0.876050  0.876050  0.876050    0.87605\nmacro avg      0.856547  0.877375  0.865671  476.00000\nweighted avg   0.882904  0.876050  0.878098  476.00000\n","output_type":"stream"}]}]}